{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import multiprocessing"
      ],
      "metadata": {
        "id": "Zp9nkpOYRZjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"â³ Downloading NLTK data...\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "print(\"âœ… NLTK data downloaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEpT539xRdas",
        "outputId": "f1cd58b8-6646-4339-db56-ad822821f711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Downloading NLTK data...\n",
            "âœ… NLTK data downloaded successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"Word embeddings are a type of word representation.\",\n",
        "    \"Word2Vec is a popular word embedding model.\",\n",
        "    \"GloVe is another word embedding model.\",\n",
        "    \"Both Word2Vec and GloVe capture semantic relationships.\"\n",
        "]\n",
        "\n",
        "print(\"ðŸ“œ Corpus defined successfully!\")\n",
        "print(\"Sample sentences:\", corpus[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H2VYn7JRgkc",
        "outputId": "2dd2f61b-8797-4b55-e57e-8f4ffea01484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“œ Corpus defined successfully!\n",
            "Sample sentences: ['Word embeddings are a type of word representation.', 'Word2Vec is a popular word embedding model.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
        "    print(\"ðŸ”¡ Tokenization successful!\")\n",
        "    print(\"Sample tokenized sentence:\", tokenized_corpus[0])\n",
        "except LookupError:\n",
        "    from nltk.tokenize import RegexpTokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokenized_corpus = [tokenizer.tokenize(sentence.lower()) for sentence in corpus]\n",
        "    print(\"âš ï¸ Used fallback tokenizer (NLTK punkt failed).\")\n",
        "    print(\"Sample tokenized sentence:\", tokenized_corpus[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y8UvHEoRleE",
        "outputId": "b03c8afd-5e33-4054-9d73-2d1aadda81cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¡ Tokenization successful!\n",
            "Sample tokenized sentence: ['word', 'embeddings', 'are', 'a', 'type', 'of', 'word', 'representation', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 100\n",
        "window_size = 5\n",
        "min_count = 1\n",
        "workers = multiprocessing.cpu_count()\n",
        "epochs = 100\n",
        "\n",
        "print(\"âš™ï¸ Word2Vec parameters set:\")\n",
        "print(f\"- Vector size: {vector_size}\")\n",
        "print(f\"- Window size: {window_size}\")\n",
        "print(f\"- Min word count: {min_count}\")\n",
        "print(f\"- Workers: {workers}\")\n",
        "print(f\"- Epochs: {epochs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtkEhUBdRqXd",
        "outputId": "3123007a-c630-4ea0-db3d-b3f1a5f61f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš™ï¸ Word2Vec parameters set:\n",
            "- Vector size: 100\n",
            "- Window size: 5\n",
            "- Min word count: 1\n",
            "- Workers: 2\n",
            "- Epochs: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(word2vec_model.wv.key_to_index.keys())\n",
        "print(f\" Vocabulary size: {len(vocab)} words\")\n",
        "print(\"Sample words in vocab:\", vocab[:5])\n",
        "\n",
        "if 'word' in word2vec_model.wv:\n",
        "    print(\"\\n Most similar words to 'word':\")\n",
        "    print(word2vec_model.wv.most_similar(\"word\", topn=3))\n",
        "else:\n",
        "    print(\"'word' not in vocabulary.\")\n",
        "if 'embedding' in word2vec_model.wv:\n",
        "    print(\"\\n Vector for 'embedding' (first 5 dims):\")\n",
        "    print(word2vec_model.wv[\"embedding\"][:5])\n",
        "else:\n",
        "    print(\"'embedding' not in vocabulary.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwXsGWAcRtoV",
        "outputId": "ead09e85-0ee2-48c6-9cce-c020a5e1f5ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Vocabulary size: 42 words\n",
            "Sample words in vocab: ['.', 'word', 'a', 'word2vec', 'by']\n",
            "\n",
            " Most similar words to 'word':\n",
            "[('.', 0.7605836987495422), ('a', 0.7580101490020752), ('representation', 0.7352306246757507)]\n",
            "\n",
            " Vector for 'embedding' (first 5 dims):\n",
            "[ 0.00836866  0.01189717  0.00362558  0.00743285 -0.00138274]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **GLOVE MODEL**"
      ],
      "metadata": {
        "id": "sATDCWz8R2RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXnxwgWESjBl",
        "outputId": "e57d5a6a-b793-4ec3-8013-dc6dc7677f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-08 05:15:31--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-05-08 05:15:31--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-05-08 05:15:31--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: â€˜glove.6B.zipâ€™\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 38s  \n",
            "\n",
            "2025-05-08 05:18:10 (5.19 MB/s) - â€˜glove.6B.zipâ€™ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "jcVppCoJSpcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embeddings(path):\n",
        "    embeddings = {}\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "glove_path = 'glove.6B.100d.txt'\n",
        "glove_embeddings = load_glove_embeddings(glove_path)"
      ],
      "metadata": {
        "id": "IFmaQbG4Vjt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Loaded {len(glove_embeddings)} word vectors\")\n",
        "print(\"Vector for 'king':\", glove_embeddings['king'][:5])\n",
        "print(\"Most similar to 'paris':\", sorted(\n",
        "    [(word, np.dot(glove_embeddings['paris'], glove_embeddings[word]))\n",
        "     for word in ['france', 'london', 'berlin']],\n",
        "    key=lambda x: -x[1]\n",
        "))"
      ],
      "metadata": {
        "id": "zlA6_aTWVmaF",
        "outputId": "e732b888-a84c-46a1-9e60-4828c28ca6fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 400000 word vectors\n",
            "Vector for 'king': [-0.32307 -0.87616  0.21977  0.25268  0.22976]\n",
            "Most similar to 'paris': [('france', 31.536636), ('london', 29.39053), ('berlin', 23.307777)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OyhneAqaVrBc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}